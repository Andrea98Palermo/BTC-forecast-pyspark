{"cells":[{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"1c0c847b-ec4b-4fa2-ba4d-0e05539b4f78","showTitle":false,"title":""}},"source":["#Import useful libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"72321f87-9ec2-4a52-bdd4-848e1d108ffc","showTitle":false,"title":""}},"outputs":[],"source":["import pyspark\n","from pyspark.sql import *\n","from pyspark.sql.types import *\n","from pyspark.sql.functions import *\n","from pyspark import SparkContext, SparkConf\n","import numpy as np \n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from datetime import datetime"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"1716945f-5ae5-446a-9fbd-389ea7f59ee2","showTitle":false,"title":""}},"source":["#Read dataset from csv\n","\n","**NB**: For some reason yahoo finance responded with 403 when trying to download the dataset directly with wget, so the file \"BTC_USD.csv\" (included in the archive) needs to be added to DBFS at \"dbfs:/FileStore/BTC_USD/BTC_USD.csv\""]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"7e11c2ec-95ef-44e0-b07d-75af6ed11894","showTitle":false,"title":""}},"outputs":[],"source":["#read from dbfs\n","df = spark.read.load(\"dbfs:/FileStore/BTC_USD/BTC_USD.csv\",\n","                      format=\"csv\", \n","                      sep=\",\", \n","                      inferSchema=\"true\", \n","                      header=\"true\");"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"d4628bb5-6ade-4928-b485-6beaf9bbcaed","showTitle":false,"title":""}},"source":["#Train-test split\n","\n","Validation not needed because CrossValidator will use part of train set as validation set (KFold)\n","\n","Proportion of split is ~ 70/30"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"30acb2cc-b20f-4551-b450-0119279c0a8f","showTitle":false,"title":""}},"outputs":[],"source":["df.createOrReplaceTempView('df_table')\n","train_set = spark.sql(\"SELECT Date, Close from df_table where Date < '20190917'\")\n","test_set = spark.sql(\"SELECT Date, Close from df_table where Date >= '20190917'\")"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"f317590a-b0f2-4dbf-8aa2-7b87915b1b69","showTitle":false,"title":""}},"source":["#Feature importance analysis\n","\n","Pearson and Spearman correlation matrices used to study the correlation between each couple of features.\n","\n","Being all features quite highly correlated, I choose to keep just one of them (close price) to be able to use a window of more days in the models"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"438464c5-6245-4781-aa9a-f6f35aa1adac","showTitle":false,"title":""}},"outputs":[],"source":["from pyspark.ml.stat import Correlation\n","from pyspark.ml.feature import VectorAssembler\n","\n","def show_matrix(matrix):\n","    \"\"\"\n","    function to print a matrix on screen\n","    \"\"\"\n","    print(matrix.collect()[0][matrix.columns[0]].toArray())\n","    print()\n","\n","vector_col = \"features\"\n","assembler = VectorAssembler(inputCols=[\"Open\", \"Close\", \"High\", \"Low\", \"Volume\"], outputCol=vector_col)\n","df_vector = assembler.transform(df).select(vector_col)\n","\n","matrix_pearson = Correlation.corr(df_vector, vector_col)    #pearson is default\n","matrix_spearman = Correlation.corr(df_vector, vector_col, \"spearman\")\n","show_matrix(matrix_pearson)\n","show_matrix(matrix_spearman)"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"d5098e7c-c149-4740-9b0c-441cd0a0a02f","showTitle":false,"title":""}},"source":["#Feature scaling\n","\n","Tanh estimator used to scale all the feature values"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"1933e99e-45cc-4d73-8093-1a2a93ede532","showTitle":false,"title":""}},"outputs":[],"source":["from pyspark.sql.functions import mean as _mean, stddev as _stddev, col, udf\n","from pyspark.sql.types import FloatType, StructType, ArrayType\n","\n","\n","train_set_stats = train_set.select(\n","    _mean(col('Close')).alias('mean'),\n","    _stddev(col('Close')).alias('std')\n",").collect()\n","mean = train_set_stats[0]['mean']    #mean of close prices\n","std = train_set_stats[0]['std']    #standard deviation of close prices\n","\n","\n","@udf(returnType=FloatType())\n","def tanh_estimator(x):\n","    \"\"\"\n","    user defined function, applies tanh estimator's formula to a feature value x\n","    \"\"\"\n","    return (float)(0.5 * (np.tanh(0.01*(x-mean)/std) + 1))\n","\n","def scale_transform(df):\n","    \"\"\"\n","    tranforms a dataframe applying tanh estimator udf\n","    \"\"\"\n","    return df.select(\"Date\", tanh_estimator(\"Close\").alias(\"Close\"))\n","\n","scaled_train_set = scale_transform(train_set)\n","scaled_test_set = scale_transform(test_set)"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"95f94644-15a8-421b-b2cf-d93646d8cfc8","showTitle":false,"title":""}},"source":["#Sliding window\n","\n","Window of 30 days is slided on the close prices in order to create train and test set, composed by examples such as: \n","\n","x={day(i), ... , day(i+29)}, y={day(i+30)}"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"e82e7b0e-5b8f-4b2d-ad02-5d32193b1ca0","showTitle":false,"title":""}},"outputs":[],"source":["from pyspark.sql.window import Window\n","\n","def slide_window(df, window_size):\n","    \"\"\"\n","    Returns two new dataframes:\n","    X - obtained sliding a window of given size (=#window_size) on the original dataframe, aggregating #window_size close prices on the same row \n","    y - for each row of X, y contains a row with the (single) price of the day after last day contained in X \n","    \"\"\"\n","    \n","    w = Window.orderBy(\"Date\")\n","    indexed_df = df.withColumn(\"Index\", row_number().over(w)).select(\"Index\", \"Close\")    #adding index to be able to loop following order and create windows\n","\n","    schema = StructType([StructField(\"Close\", ArrayType(FloatType()), False)])   #schema for X (array of floats)\n","\n","    X = spark.createDataFrame(spark.sparkContext.emptyRDD(), schema )\n","    y = spark.createDataFrame(spark.sparkContext.emptyRDD(), FloatType())\n","\n","    length = indexed_df.count()\n","    for i in range(window_size+1, length+1):\n","        new_df = indexed_df.where(col(\"Index\").between(i-window_size, i-1)).select(\"Close\")    #select the window\n","        new_row = new_df.agg(collect_list(\"Close\").alias(\"Close\"))    #create new X's row with all prices from window\n","        X = X.union(new_row)\n","        new_row = indexed_df.where(col(\"Index\") == i).select(\"Close\")    #create new Y's row with price of the day after last day contained in X \n","        y = y.union(new_row)\n","    \n","    return X, y"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"f00cd06f-90b5-471b-a9fb-9f08c28a8961","showTitle":false,"title":""}},"outputs":[],"source":["window = 30    #window size\n","\n","X_train, y_train = slide_window(scaled_train_set, window)    #slide window on train set\n","X_test, y_test = slide_window(scaled_test_set, window)    #slide window on test set"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"5a1d79f5-eaff-4b74-856c-9553bb7981c6","showTitle":false,"title":""}},"source":["#Merging X and y\n","\n","X and y (for both train and test) need to be merged as the Pyspark regression models require them in a single dataframe"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"d6c40440-e2cb-4948-bcf8-2b51fd9c1a10","showTitle":false,"title":""}},"outputs":[],"source":["def merge_X_y(X, y):\n","    \"\"\"\n","    merges two dataframes column-wise\n","    \"\"\"\n","    schema = StructType(X.schema.fields + y.schema.fields)\n","    X_y = X.rdd.zip(y.rdd).map(lambda x: x[0]+x[1])\n","    return spark.createDataFrame(X_y, schema)\n","\n","X_y_train = merge_X_y(X_train, y_train)\n","X_y_test = merge_X_y(X_test, y_test)"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"b0f4aa5c-3af5-4be1-b1d8-ec44f2be3d94","showTitle":false,"title":""}},"source":["#Vectorization of windows\n","\n","Windows represented as lists of days need to be converted to vectors (rows) of features as Pyspark regression models require dataframes in this form"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"1e5e3a95-7a5e-42ff-b132-09368b85d451","showTitle":false,"title":""}},"outputs":[],"source":["from pyspark.ml.linalg import Vectors, VectorUDT\n","from pyspark.sql.functions import udf\n","from pyspark import StorageLevel\n","\n","list_to_vector_udf = udf(lambda l: Vectors.dense(l), VectorUDT())    #converts list of prices to features vector \n","\n","def assemble_window(X_y):\n","    \"\"\"\n","    applies list_to_vector_udf to given dataframe\n","    \"\"\"\n","    return X_y.select(list_to_vector_udf(X_y[\"Close\"]).alias(\"features\"), X_y[\"value\"].alias(\"label\"))\n","\n","X_y_train_vec = assemble_window(X_y_train)\n","X_y_test_vec = assemble_window(X_y_test)"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"614730a4-1ece-42ee-a26e-143b3844b6cb","showTitle":false,"title":""}},"source":["#Hyperparameter tuning/model selection/evaluation\n","\n","In this section linear regression and gradient-boosted trees regression models are cross-validated partitioning the train set in 3 folds in order to tune their hyperparameters and find the best model.\n","\n","Then the best models found are tested on unseen data (test set) and the actual and predicted prices are plotted."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"6ba152ac-29f5-42a7-92bb-8fd9b1e30877","showTitle":false,"title":""}},"outputs":[],"source":["from pyspark.ml.tuning import ParamGridBuilder, CrossValidator, CrossValidatorModel\n","from pyspark.ml.evaluation import RegressionEvaluator\n","\n","def cross_validate(model, param_grid, df):\n","    \"\"\"\n","    Performs grid search on given model with given parameter grid, using given dataframe as train/validation data.\n","    Returns the validated model (ready to be used for predictions using best parameters found)\n","    \"\"\"\n","    evaluator = RegressionEvaluator(metricName=\"rmse\")\n","    cv = CrossValidator(estimator=model, estimatorParamMaps=param_grid, evaluator=evaluator)\n","    validated_model = cv.fit(df)\n","    return validated_model"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"9e1af3dd-e416-47a9-8b1b-2190ae6471a9","showTitle":false,"title":""}},"source":["#Linear Regression"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"8ee1ff98-5f70-47a4-ba64-5a0c9c42cea6","showTitle":false,"title":""}},"outputs":[],"source":["from pyspark.ml.regression import LinearRegression\n","\n","evaluator = RegressionEvaluator(metricName=\"rmse\")\n","\n","lr = LinearRegression(standardization=False)    #avoid standardization as tanh estimator has been applied yet\n","param_grid = ParamGridBuilder().addGrid(lr.regParam, [0.33, 0.66]).addGrid(lr.elasticNetParam, [0.33, 0.5, 0.66]).build()    #parameters to be tuned\n","validated_lr_model = cross_validate(lr, param_grid, X_y_train_vec)\n","\n","#best parameters found\n","elasticNet = validated_lr_model.bestModel.getElasticNetParam()\n","reg = validated_lr_model.bestModel.getRegParam()\n","\n","print(\"ElasicNetParam of best model -> \", elasticNet)\n","print(\"RegParam of best model -> = \", reg)\n","\n","predictions = validated_lr_model.transform(X_y_test_vec)    #test on unseen data\n","RMSE = evaluator.evaluate(predictions)    #evaluate predictions using ROOT MEAN SQUARED ERROR\n","print(\"RMSE of best model on unseen data -> \", RMSE)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"3f287603-ac03-4b73-89da-3b59f4569fcc","showTitle":false,"title":""}},"outputs":[],"source":["def plot_predictions(predictions):\n","    \"\"\"\n","    plots two lines representing predicted and actual prices\n","    \"\"\"\n","    pandas_df = predictions.select('label', 'prediction').toPandas()\n","    plt.figure(figsize=(20, 7))\n","    plt.plot(range(len(pandas_df['label'].values)), pandas_df['label'].values, label = 'Actual Price', color = 'blue')\n","    plt.plot(range(len(pandas_df['prediction'].values)), pandas_df['prediction'].values, label = 'Predicted Price', color = 'red')\n","    plt.xticks(np.arange(100, pandas_df.shape[0], 200))\n","    plt.xlabel('Time')\n","    plt.ylabel('Price (scaled)')\n","    plt.legend()\n","    plt.show()\n","    \n","plot_predictions(predictions)    #plot linear regression's predictions"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"1bf12cf4-b268-4475-9f3e-56e5166ba361","showTitle":false,"title":""}},"source":["#Gradient-boosted trees"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"84e59dab-0bb1-40b8-9cde-6147793b8e78","showTitle":false,"title":""}},"outputs":[],"source":["from pyspark.ml.regression import GBTRegressor\n","\n","gbt_r = GBTRegressor()\n","param_grid = ParamGridBuilder().addGrid(gbt_r.maxDepth, [4, 8, 12]).addGrid(gbt_r.featureSubsetStrategy, ['0.33', '0.66']).build()    #parameters to be tuned\n","validated_gbt_model = cross_validate(gbt_r, param_grid, X_y_train_vec)\n","\n","#best parameters found\n","max_depth = validated_gbt_model.bestModel.getMaxDepth()\n","subsample = validated_gbt_model.bestModel.getFeatureSubsetStrategy()\n","\n","print(\"maxDepth of best model -> \", max_depth)\n","print(\"featureSubsetStrategy of best model -> = \", subsample)\n","\n","predictions = validated_gbt_model.transform(X_y_test_vec)    #test on unseen data\n","RMSE = evaluator.evaluate(predictions)    #evaluate predictions using ROOT MEAN SQUARED ERROR\n","print(\"RMSE of best model on unseen data ->  \", RMSE)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"a64c1b54-9589-4dd4-bfbe-97c91b88c34e","showTitle":false,"title":""}},"outputs":[],"source":["plot_predictions(predictions)    #plot gradient-boosted trees' predicitons"]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"pythonIndentUnit":4},"notebookName":"Project","notebookOrigID":54062525639786,"widgets":{}},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
